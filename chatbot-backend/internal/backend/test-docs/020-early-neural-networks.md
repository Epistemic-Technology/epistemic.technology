# The Dawn of Neural Networks: 1943-1989

*Published: 2024-01-30*

The history of neural networks begins with the groundbreaking 1943 paper by McCulloch and Pitts, who proposed the first mathematical model of a neural network using propositional logic. Their binary threshold unit, processing inputs of 1s and 0s, laid the foundation for modern artificial neurons, though it could only handle 7 logical operations per second on the vacuum tube computers of the time.

## The Perceptron Era

Frank Rosenblatt's Mark 1 Perceptron, built in 1957 at Cornell Aeronautical Laboratory, represented the first hardware implementation of a neural network. The machine, weighing 5 tons and using motor-driven potentiometers for weight adjustments, could process a 20x20 pixel image using 512 photocells connected to 8 layers of artificial neurons. Despite its limitations, it achieved 70% accuracy in distinguishing simple shapes.

## Parallel Distributed Processing

The PDP Research Group's work in 1986 revolutionized the field with their implementation of backpropagation. Running on a VAX-11/780 computer with 8MB of RAM, their network could train on 100 pattern recognition tasks in 12 hours, achieving an unprecedented 85% accuracy on handwritten digit recognition using a three-layer network with 64 hidden units.

## Early Commercial Applications

The first commercial neural network product, the Nestor Learning System (1988), ran on an Intel 80386 processor at 16MHz. It could process 50 inputs through a single hidden layer of 30 neurons, achieving character recognition rates of 300 characters per minute with 92% accuracy, requiring 4MB of RAM and 40MB of hard disk space for training data storage.

## Theoretical Breakthroughs

Hopfield's 1982 paper on recurrent neural networks introduced a network architecture requiring only nÂ² connections for n neurons, dramatically reducing computational requirements. His implementation on a DEC PDP-11 computer could store and retrieve 15 distinct patterns using 100 neurons, with a memory capacity of 0.15n patterns for n neurons.

The evolution of neural networks was marked by both hardware constraints and theoretical insights. The NEF-1 supercomputer of 1989, dedicated to neural network research, could simulate networks with up to 1 million connections at 150 million connection updates per second, a remarkable achievement that helped validate many early theoretical predictions about neural network scaling. 